Freedom vs Constraints: Investigating Autonomy in LLM Code Evaluation through Cognitive Science Principles

Shaked Goldstein, Niv Arad, Prof. Assaf Schuster


Abstract

Research in cognitive science has shown that humans often achieve better outcomes when granted greater freedom of choice, with autonomy linked to improved motivation and decision quality1. This study investigates whether a similar principle applies to large language models (LLMs) when evaluating and generating code. We compare two prompting conditions: a structured mode, where the model is instructed to select/generate code snippets based on explicit criteria (e.g., time or memory efficiency), and a freedom mode, where the model chooses or creates the “best” snippet without imposed constraints. To test this, we use a curated set of programming problems spanning multiple difficulty levels, drawn from the publicly available benchmark “APPS”, supplemented with custom tasks designed to introduce meaningful trade-offs. Model performance was evaluated primarily through automated test-case execution, using pass rate as the main metric. Preliminary findings suggest that, much like the human mind, granting LLMs greater freedom of choice during code generation leads to more robust, adaptable, and higher-quality solutions, offering insight into whether LLMs exhibit human-like benefits from autonomy in decision-making and what this implies for prompt engineering and AI alignment.
